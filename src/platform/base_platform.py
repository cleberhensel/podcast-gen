from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

class BasePlatform(ABC):
    """Interface base para otimizaÃ§Ãµes especÃ­ficas de plataforma"""
    
    def __init__(self):
        from .detector import PlatformDetector
        self.platform_info = PlatformDetector.get_platform_info()
        self.is_optimized = False
        
        logger.info(f"ðŸ”§ Plataforma inicializada: {self.platform_info['platform_type'].value}")
    
    @abstractmethod
    def optimize_pytorch(self):
        """OtimizaÃ§Ãµes especÃ­ficas do PyTorch para esta plataforma"""
        pass
    
    @abstractmethod
    def cleanup_gpu_memory(self):
        """Limpeza de memÃ³ria GPU especÃ­fica da plataforma"""
        pass
    
    @abstractmethod
    def get_optimal_workers(self) -> int:
        """NÃºmero ideal de workers para esta plataforma"""
        pass
    
    @abstractmethod
    def get_memory_threshold(self) -> float:
        """Threshold de memÃ³ria para cleanup agressivo (0.0-1.0)"""
        pass
    
    @abstractmethod
    def configure_audio(self) -> Dict[str, Any]:
        """ConfiguraÃ§Ãµes de Ã¡udio especÃ­ficas da plataforma"""
        pass
    
    @abstractmethod
    def get_batch_size(self) -> int:
        """Tamanho ideal de batch para processamento"""
        pass
    
    def get_performance_config(self) -> Dict[str, Any]:
        """ConfiguraÃ§Ã£o de performance geral"""
        return {
            'max_workers': self.get_optimal_workers(),
            'memory_threshold': self.get_memory_threshold(),
            'batch_size': self.get_batch_size(),
            'platform_type': self.platform_info['platform_type'].value,
            'gpu_backend': self.platform_info['gpu_backend'].value,
            'audio_config': self.configure_audio(),
            'is_optimized': self.is_optimized
        }
    
    def initialize_optimizations(self):
        """Inicializa todas as otimizaÃ§Ãµes da plataforma"""
        try:
            logger.info(f"ðŸš€ Inicializando otimizaÃ§Ãµes para {self.platform_info['platform_type'].value}...")
            
            # Aplicar otimizaÃ§Ãµes PyTorch
            self.optimize_pytorch()
            
            # Configurar Ã¡udio
            audio_config = self.configure_audio()
            logger.debug(f"Ãudio configurado: {audio_config}")
            
            self.is_optimized = True
            logger.info("âœ… OtimizaÃ§Ãµes aplicadas com sucesso!")
            
        except Exception as e:
            logger.error(f"âŒ Erro aplicando otimizaÃ§Ãµes: {e}")
            self.is_optimized = False
            raise
    
    def get_system_status(self) -> Dict[str, Any]:
        """Status atual do sistema"""
        try:
            import psutil
            
            status = {
                'cpu_percent': psutil.cpu_percent(interval=1),
                'memory_percent': psutil.virtual_memory().percent,
                'available_memory_gb': psutil.virtual_memory().available / (1024**3),
                'is_optimized': self.is_optimized
            }
            
            # InformaÃ§Ãµes de GPU se disponÃ­vel
            gpu_backend = self.platform_info['gpu_backend']
            if gpu_backend.value == 'mps':
                # MPS nÃ£o tem APIs de monitoramento direto
                status['gpu_available'] = True
                status['gpu_type'] = 'Metal Performance Shaders'
            elif gpu_backend.value == 'cuda':
                try:
                    import torch
                    if torch.cuda.is_available():
                        status['gpu_available'] = True
                        status['gpu_memory_allocated'] = torch.cuda.memory_allocated() / (1024**3)
                        status['gpu_memory_reserved'] = torch.cuda.memory_reserved() / (1024**3)
                        status['gpu_type'] = 'NVIDIA CUDA'
                    else:
                        status['gpu_available'] = False
                except:
                    status['gpu_available'] = False
            else:
                status['gpu_available'] = False
                status['gpu_type'] = 'CPU-only'
            
            return status
            
        except Exception as e:
            logger.error(f"Erro obtendo status do sistema: {e}")
            return {'error': str(e)}
    
    def should_cleanup_memory(self) -> bool:
        """Determina se deve fazer cleanup baseado no threshold"""
        try:
            import psutil
            memory_percent = psutil.virtual_memory().percent / 100.0
            threshold = self.get_memory_threshold()
            
            return memory_percent > threshold
            
        except Exception:
            # Em caso de erro, sempre fazer cleanup
            return True
    
    def log_performance_summary(self):
        """Log resumo de performance atual"""
        try:
            status = self.get_system_status()
            config = self.get_performance_config()
            
            logger.info(f"ðŸ“Š PERFORMANCE SUMMARY:")
            logger.info(f"   â€¢ Platform: {config['platform_type']}")
            logger.info(f"   â€¢ GPU: {config['gpu_backend']}")
            logger.info(f"   â€¢ Workers: {config['max_workers']}")
            logger.info(f"   â€¢ Batch Size: {config['batch_size']}")
            logger.info(f"   â€¢ Memory Threshold: {config['memory_threshold']:.1%}")
            logger.info(f"   â€¢ CPU Usage: {status.get('cpu_percent', 'N/A'):.1f}%")
            logger.info(f"   â€¢ Memory Usage: {status.get('memory_percent', 'N/A'):.1f}%")
            
            if status.get('gpu_available'):
                logger.info(f"   â€¢ GPU Type: {status.get('gpu_type', 'Unknown')}")
                if 'gpu_memory_allocated' in status:
                    logger.info(f"   â€¢ GPU Memory: {status['gpu_memory_allocated']:.1f}GB")
                    
        except Exception as e:
            logger.error(f"Erro no log de performance: {e}") 